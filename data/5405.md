# Preface

These notes, and the freshman-level university course they support, are about information. Although you may have a general idea of what information is, you may not realize that the information you deal with can be quantified. That’s right, you can measure the amount of information and use general principles about how information behaves. We will apply these ideas to computation and to communications, and also look at general laws in other fields of science and engineering.

One of these general laws is the Second Law of Thermodynamics. Although thermodynamics, a branch of physics, deals with physical systems, the Second Law is developed here in the context of information pro-cessing in natural and engineered systems. The Second Law as traditionally stated in thermodynamics deals with a physical quantity known as “entropy.” Everybody has heard of entropy, but few really understand it. Only recently has entropy been thought of as a form of information.

The Second Law is surely one of science’s most glorious achievements. However, it is usually taught in the context of physical systems used for processing energy, making it hard for beginners to appreciate. On the other hand, the forms of the Second Law that apply to computation and communications (where information is processed) are more easily understood, especially today as the information revolution is under way.

These notes and the course based on them are intended for first-year university students. Although they were developed at the Massachusetts Institute of Technology, a university that specializes in science and engineering, and one at which a full year of calculus is required for all undergraduates, calculus is not used much at all in these notes. Most examples are from discrete systems, not continuous systems, so that sums and differences can be used instead of integrals and derivatives. In other words, algebra is used instead of calculus. This course should be accessible to university students who are not studying engineering or science, and even to well prepared high-school students. In fact, this course could, when combined with a similar one about energy, provide excellent science background for liberal-arts students.

The analogy between information and energy is interesting. In both high-school and first-year college physics courses, students learn about the physical quantity called energy which can be found in various places at various times in various forms (potential, kinetic, electric, chemical, etc.). Energy can exist in one region of space or another, can flow from one place to another, can be stored for later use, and can be converted from one form to another. But most important, energy is conserved—despite its being moved, stored, and converted, at the end of the day there is still exactly the same total amount of energy.

This conservation of energy principle is sometimes known as the First Law of Thermodynamics. It has proven to be so important and fundamental that whenever a “leak” was found, the theory was rescued by defining a new form of energy. One example of this occurred in 1905 when Albert Einstein recognized that mass is a form of energy, as expressed by his famous formula E = mc^2^. That understanding later enabled the development of devices (atomic bombs and nuclear power plants) that convert energy from its form as mass to other forms.

But what about information? If entropy is really a form of information, there should be a theory that covers both and describes how information can be turned into entropy or vice versa. Such a theory is not yet well developed, for several historical reasons. But that is exactly what is needed to simplify the teaching and understanding of fundamental concepts, particularly the Second Law of Thermodynamics.

These notes present such a unified view of information, in which entropy is one form of information, but in which there are other forms as well. Like energy, information can reside in one place or another, it can be transmitted through space, and it can be stored for later use. But unlike energy, information is not conserved: the Second Law states that entropy never decreases as time goes on—it generally increases, but may in special cases stay constant. Also, information is inherently subjective, because it deals with what you know and what you don’t know (entropy, since it is one form of information, is also subjective—this point makes some physicists uneasy). These two facts make the theory of information different from theories that deal with conserved quantities such as energy—different and also interesting.

The unified framework presented here has never before been developed specifically for freshmen. It is not entirely consistent with conventional thinking in various disciplines, which were developed separately. In fact, we may not yet have it right. One consequence of teaching at an elementary level is that unanswered questions close at hand are disturbing, and their resolution demands new research into the fundamentals. Trying to explain things rigorously but simply often requires new organizing principles and new approaches. In the present case, the new approach is to start with information and work from there to entropy, and the new organizing principle is the unified theory of information.

This will be an exciting journey. Welcome aboard!
